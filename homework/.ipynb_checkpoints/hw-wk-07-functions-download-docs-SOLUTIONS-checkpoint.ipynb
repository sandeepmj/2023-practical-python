{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a089baa",
   "metadata": {},
   "source": [
    "# 1. Demo downloading files from websites \n",
    "\n",
    "There are ```txt``` and ```pdf``` files on:\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/pages.html```\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Download all ```pdf``` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2151394",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create cells here AS needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e927e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup  ## scrape info from web pages\n",
    "import requests ## get web pages from server\n",
    "import time # time is required. we will use its sleep function\n",
    "from random import randrange # generate random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dea1c38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url to scrape\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/pages.html\"\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f59faeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html lang=\"en\">\n",
       "<head>\n",
       "<!-- Makes the page responsive and scaled to be read easily -->\n",
       "<meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
       "<!-- Links to stylesheet -->\n",
       "<link href=\"style.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
       "<!-- Remember to update page title -->\n",
       "<title>List of Documents</title>\n",
       "</head>\n",
       "<body>\n",
       "<!-- All content goes here -->\n",
       "<div class=\"container\">\n",
       "<h1>Documents to Download</h1>\n",
       "<li>Junk Li <a href=\"\">tag 1</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 2</a></li>\n",
       "<ul class=\"txts downloadable\">\n",
       "<p class=\"pages\">Download this first set of text documents</p>\n",
       "<li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
       "<li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
       "</ul>\n",
       "<li>Junk Li <a href=\"\">tag 3</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 4</a></li>\n",
       "<ul class=\"pdfs downloadable\">\n",
       "<p class=\"pages\">Download this list of PDFs</p>\n",
       "<li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
       "<li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
       "<li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
       "</ul>\n",
       "<li>Junk Li <a href=\"\">tag 5</a></li>\n",
       "<li>Junk Li <a href=\"\">tag 6</a></li>\n",
       "<ul class=\"txts downloadable\">\n",
       "<p class=\"pages\">Download this second set of text documents</p>\n",
       "<li>Text Document <a href=\"files/text_doc_A.txt\">1</a> </li>\n",
       "<li>Text Document <a href=\"files/text_doc_B.txt\">2</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_C.txt\">3</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_D.txt\">4</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_E.txt\">5</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_F.txt\">6</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_G.txt\">7</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_H.txt\">8</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_I.txt\">9</a></li>\n",
       "<li>Text Document <a href=\"files/text_doc_J.txt\">10</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## GET SOUP\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1dac660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       " <a href=\"files/text_doc_02.txt\">2</a>,\n",
       " <a href=\"files/text_doc_03.txt\">3</a>,\n",
       " <a href=\"files/text_doc_04.txt\">4</a>,\n",
       " <a href=\"files/text_doc_05.txt\">5</a>,\n",
       " <a href=\"files/text_doc_06.txt\">6</a>,\n",
       " <a href=\"files/text_doc_07.txt\">7</a>,\n",
       " <a href=\"files/text_doc_08.txt\">8</a>,\n",
       " <a href=\"files/text_doc_09.txt\">9</a>,\n",
       " <a href=\"files/text_doc_10.txt\">10</a>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## save in list called txt_holder\n",
    "a_tags = soup.find(\"ul\", class_=\"txts\").find_all(\"a\")\n",
    "a_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa5d5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## base url\n",
    "base_url = \"https://sandeepmj.github.io/scrape-example-page/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f00bdb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/text_doc_01.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_02.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_03.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_04.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_05.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_06.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_07.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_08.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_09.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_10.txt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## look at the links\n",
    "links = []\n",
    "for a_tag in a_tags:\n",
    "    links.append(base_url + a_tag.get(\"href\"))\n",
    "\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01d9fa6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /Users/sandeepjunnarkar/opt/anaconda3/lib/python3.9/site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c46c6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget # can put down documents, files from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c26dc958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 10\n",
      "100% [................................................................] 76 / 76Snoozing for 6 seconds next link\n",
      "Downloading link 2 of 10\n",
      "100% [................................................................] 66 / 66Snoozing for 4 seconds next link\n",
      "Downloading link 3 of 10\n",
      "100% [................................................................] 70 / 70Snoozing for 5 seconds next link\n",
      "Downloading link 4 of 10\n",
      "100% [................................................................] 63 / 63Snoozing for 6 seconds next link\n",
      "Downloading link 5 of 10\n",
      "100% [................................................................] 66 / 66Snoozing for 5 seconds next link\n",
      "Downloading link 6 of 10\n",
      "100% [................................................................] 66 / 66Snoozing for 3 seconds next link\n",
      "Downloading link 7 of 10\n",
      "100% [................................................................] 69 / 69Snoozing for 3 seconds next link\n",
      "Downloading link 8 of 10\n",
      "100% [................................................................] 65 / 65Snoozing for 4 seconds next link\n",
      "Downloading link 9 of 10\n",
      "100% [................................................................] 66 / 66Snoozing for 4 seconds next link\n",
      "Downloading link 10 of 10\n",
      "100% [................................................................] 60 / 60Snoozing for 4 seconds next link\n",
      "Done downloading all docs\n"
     ]
    }
   ],
   "source": [
    "## download with timer\n",
    "\n",
    "link_count = 1\n",
    "\n",
    "\n",
    "for link in links:\n",
    "    print(f\"Downloading link {link_count} of {len(links)}\")\n",
    "    link_count += 1\n",
    "    wget.download(link)\n",
    "    snoozer = randrange(3, 7)\n",
    "    print(f\"Snoozing for {snoozer} seconds next link\")\n",
    "    time.sleep(snoozer)\n",
    "print(\"Done downloading all docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc37c3dc",
   "metadata": {},
   "source": [
    "# 2. Demo downloading ALL files from websites \n",
    "\n",
    "There are ```txt``` and ```pdf``` files on:\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/pages.html```\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Download all  files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d786775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create cells here AS needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2248a75f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ul class=\"txts downloadable\">\n",
       " <p class=\"pages\">Download this first set of text documents</p>\n",
       " <li>Text Document <a href=\"files/text_doc_01.txt\">1</a> </li>\n",
       " <li>Text Document <a href=\"files/text_doc_02.txt\">2</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_03.txt\">3</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_04.txt\">4</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_05.txt\">5</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_06.txt\">6</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_07.txt\">7</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_08.txt\">8</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_09.txt\">9</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_10.txt\">10</a></li>\n",
       " </ul>,\n",
       " <ul class=\"pdfs downloadable\">\n",
       " <p class=\"pages\">Download this list of PDFs</p>\n",
       " <li>PDF Document <a href=\"files/pdf_1.pdf\">1</a> </li>\n",
       " <li>PDF Document <a href=\"files/pdf_2.pdf\">2</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_3.pdf\">3</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_4.pdf\">4</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_5.pdf\">5</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_6.pdf\">6</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_7.pdf\">7</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_8.pdf\">8</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_9.pdf\">9</a></li>\n",
       " <li>PDF Document <a href=\"files/pdf_10.pdf\">10</a></li>\n",
       " </ul>,\n",
       " <ul class=\"txts downloadable\">\n",
       " <p class=\"pages\">Download this second set of text documents</p>\n",
       " <li>Text Document <a href=\"files/text_doc_A.txt\">1</a> </li>\n",
       " <li>Text Document <a href=\"files/text_doc_B.txt\">2</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_C.txt\">3</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_D.txt\">4</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_E.txt\">5</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_F.txt\">6</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_G.txt\">7</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_H.txt\">8</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_I.txt\">9</a></li>\n",
       " <li>Text Document <a href=\"files/text_doc_J.txt\">10</a></li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## find all links to files in our soup\n",
    "all_files = soup.find_all(\"ul\", class_=\"downloadable\")\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20963df5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       "  <a href=\"files/text_doc_02.txt\">2</a>,\n",
       "  <a href=\"files/text_doc_03.txt\">3</a>,\n",
       "  <a href=\"files/text_doc_04.txt\">4</a>,\n",
       "  <a href=\"files/text_doc_05.txt\">5</a>,\n",
       "  <a href=\"files/text_doc_06.txt\">6</a>,\n",
       "  <a href=\"files/text_doc_07.txt\">7</a>,\n",
       "  <a href=\"files/text_doc_08.txt\">8</a>,\n",
       "  <a href=\"files/text_doc_09.txt\">9</a>,\n",
       "  <a href=\"files/text_doc_10.txt\">10</a>],\n",
       " [<a href=\"files/pdf_1.pdf\">1</a>,\n",
       "  <a href=\"files/pdf_2.pdf\">2</a>,\n",
       "  <a href=\"files/pdf_3.pdf\">3</a>,\n",
       "  <a href=\"files/pdf_4.pdf\">4</a>,\n",
       "  <a href=\"files/pdf_5.pdf\">5</a>,\n",
       "  <a href=\"files/pdf_6.pdf\">6</a>,\n",
       "  <a href=\"files/pdf_7.pdf\">7</a>,\n",
       "  <a href=\"files/pdf_8.pdf\">8</a>,\n",
       "  <a href=\"files/pdf_9.pdf\">9</a>,\n",
       "  <a href=\"files/pdf_10.pdf\">10</a>],\n",
       " [<a href=\"files/text_doc_A.txt\">1</a>,\n",
       "  <a href=\"files/text_doc_B.txt\">2</a>,\n",
       "  <a href=\"files/text_doc_C.txt\">3</a>,\n",
       "  <a href=\"files/text_doc_D.txt\">4</a>,\n",
       "  <a href=\"files/text_doc_E.txt\">5</a>,\n",
       "  <a href=\"files/text_doc_F.txt\">6</a>,\n",
       "  <a href=\"files/text_doc_G.txt\">7</a>,\n",
       "  <a href=\"files/text_doc_H.txt\">8</a>,\n",
       "  <a href=\"files/text_doc_I.txt\">9</a>,\n",
       "  <a href=\"files/text_doc_J.txt\">10</a>]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## iterate through our list\n",
    "## to find just the a tags \n",
    "all_a_tags = [file.find_all(\"a\") for file in all_files]\n",
    "all_a_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d388ee",
   "metadata": {},
   "source": [
    "### We need to flatten all_files so it contains indivudal items rather than multiple lists.\n",
    "\n",
    "We'll use ```itertools``` to accomplish that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51e3f735",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import itertools\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f683272d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"files/text_doc_01.txt\">1</a>,\n",
       " <a href=\"files/text_doc_02.txt\">2</a>,\n",
       " <a href=\"files/text_doc_03.txt\">3</a>,\n",
       " <a href=\"files/text_doc_04.txt\">4</a>,\n",
       " <a href=\"files/text_doc_05.txt\">5</a>,\n",
       " <a href=\"files/text_doc_06.txt\">6</a>,\n",
       " <a href=\"files/text_doc_07.txt\">7</a>,\n",
       " <a href=\"files/text_doc_08.txt\">8</a>,\n",
       " <a href=\"files/text_doc_09.txt\">9</a>,\n",
       " <a href=\"files/text_doc_10.txt\">10</a>,\n",
       " <a href=\"files/pdf_1.pdf\">1</a>,\n",
       " <a href=\"files/pdf_2.pdf\">2</a>,\n",
       " <a href=\"files/pdf_3.pdf\">3</a>,\n",
       " <a href=\"files/pdf_4.pdf\">4</a>,\n",
       " <a href=\"files/pdf_5.pdf\">5</a>,\n",
       " <a href=\"files/pdf_6.pdf\">6</a>,\n",
       " <a href=\"files/pdf_7.pdf\">7</a>,\n",
       " <a href=\"files/pdf_8.pdf\">8</a>,\n",
       " <a href=\"files/pdf_9.pdf\">9</a>,\n",
       " <a href=\"files/pdf_10.pdf\">10</a>,\n",
       " <a href=\"files/text_doc_A.txt\">1</a>,\n",
       " <a href=\"files/text_doc_B.txt\">2</a>,\n",
       " <a href=\"files/text_doc_C.txt\">3</a>,\n",
       " <a href=\"files/text_doc_D.txt\">4</a>,\n",
       " <a href=\"files/text_doc_E.txt\">5</a>,\n",
       " <a href=\"files/text_doc_F.txt\">6</a>,\n",
       " <a href=\"files/text_doc_G.txt\">7</a>,\n",
       " <a href=\"files/text_doc_H.txt\">8</a>,\n",
       " <a href=\"files/text_doc_I.txt\">9</a>,\n",
       " <a href=\"files/text_doc_J.txt\">10</a>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use itertools to flatten the list\n",
    "\n",
    "flat_list = list(itertools.chain(*all_a_tags))\n",
    "flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44eb5312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/files/text_doc_01.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_02.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_03.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_04.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_05.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_06.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_07.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_08.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_09.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_10.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_1.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_2.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_3.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_4.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_5.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_6.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_7.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_8.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_9.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/pdf_10.pdf',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_A.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_B.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_C.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_D.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_E.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_F.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_G.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_H.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_I.txt',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/files/text_doc_J.txt']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## now we can target the a tags\n",
    "\n",
    "links = [base_url + a_tag.get(\"href\") for a_tag in flat_list]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e979f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading link 1 of 30\n",
      "100% [................................................................] 76 / 76Snoozing for 4 seconds next link\n",
      "Downloading link 2 of 30\n",
      "100% [................................................................] 66 / 66Snoozing for 6 seconds next link\n",
      "Downloading link 3 of 30\n",
      "100% [................................................................] 70 / 70Snoozing for 5 seconds next link\n",
      "Downloading link 4 of 30\n",
      "100% [................................................................] 63 / 63Snoozing for 3 seconds next link\n",
      "Downloading link 5 of 30\n",
      "100% [................................................................] 66 / 66Snoozing for 5 seconds next link\n",
      "Downloading link 6 of 30\n",
      "100% [................................................................] 66 / 66Snoozing for 6 seconds next link\n",
      "Downloading link 7 of 30\n",
      "100% [................................................................] 69 / 69Snoozing for 5 seconds next link\n",
      "Downloading link 8 of 30\n",
      "100% [................................................................] 65 / 65Snoozing for 3 seconds next link\n",
      "Downloading link 9 of 30\n",
      "100% [................................................................] 66 / 66Snoozing for 6 seconds next link\n",
      "Downloading link 10 of 30\n",
      "100% [................................................................] 60 / 60Snoozing for 6 seconds next link\n",
      "Downloading link 11 of 30\n",
      "100% [..........................................................] 12812 / 12812Snoozing for 3 seconds next link\n",
      "Downloading link 12 of 30\n",
      "100% [..........................................................] 12897 / 12897Snoozing for 3 seconds next link\n",
      "Downloading link 13 of 30\n",
      "100% [..........................................................] 12908 / 12908Snoozing for 4 seconds next link\n",
      "Downloading link 14 of 30\n",
      "100% [..........................................................] 12843 / 12843Snoozing for 6 seconds next link\n",
      "Downloading link 15 of 30\n",
      "100% [..........................................................] 12881 / 12881Snoozing for 6 seconds next link\n",
      "Downloading link 16 of 30\n",
      "100% [..........................................................] 12906 / 12906Snoozing for 4 seconds next link\n",
      "Downloading link 17 of 30\n",
      "100% [..........................................................] 12816 / 12816Snoozing for 4 seconds next link\n",
      "Downloading link 18 of 30\n",
      "100% [..........................................................] 12921 / 12921Snoozing for 3 seconds next link\n",
      "Downloading link 19 of 30\n",
      "100% [..........................................................] 12901 / 12901Snoozing for 3 seconds next link\n",
      "Downloading link 20 of 30\n",
      "100% [..........................................................] 13049 / 13049Snoozing for 4 seconds next link\n",
      "Downloading link 21 of 30\n",
      "100% [................................................................] 76 / 76Snoozing for 5 seconds next link\n",
      "Downloading link 22 of 30\n",
      "100% [................................................................] 66 / 66Snoozing for 3 seconds next link\n",
      "Downloading link 23 of 30\n",
      "100% [................................................................] 70 / 70Snoozing for 3 seconds next link\n",
      "Downloading link 24 of 30\n",
      "100% [................................................................] 63 / 63Snoozing for 6 seconds next link\n",
      "Downloading link 25 of 30\n",
      "100% [................................................................] 66 / 66Snoozing for 4 seconds next link\n",
      "Downloading link 26 of 30\n",
      "100% [................................................................] 66 / 66Snoozing for 6 seconds next link\n",
      "Downloading link 27 of 30\n",
      "100% [................................................................] 69 / 69Snoozing for 3 seconds next link\n",
      "Downloading link 28 of 30\n",
      "100% [................................................................] 65 / 65Snoozing for 5 seconds next link\n",
      "Downloading link 29 of 30\n",
      "100% [................................................................] 66 / 66Snoozing for 5 seconds next link\n",
      "Downloading link 30 of 30\n",
      "100% [................................................................] 60 / 60Snoozing for 4 seconds next link\n",
      "Done downloading all docs\n"
     ]
    }
   ],
   "source": [
    "link_count = 1\n",
    "\n",
    "\n",
    "for link in links:\n",
    "    print(f\"Downloading link {link_count} of {len(links)}\")\n",
    "    link_count += 1\n",
    "    wget.download(link)\n",
    "    snoozer = randrange(3, 7)\n",
    "    print(f\"Snoozing for {snoozer} seconds next link\")\n",
    "    time.sleep(snoozer)\n",
    "print(\"Done downloading all docs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9faeae",
   "metadata": {},
   "source": [
    "# 3. Conversion function\n",
    "\n",
    "\n",
    "Write a function that takes string values like ```$12.24```, ```10,201.7654``` and ```$12,501``` and converts them into floating point numbers like ```12.24```, ```10201.77``` and ```12501.0```\n",
    "\n",
    "Test it out on those 3 string values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca97e315",
   "metadata": {},
   "outputs": [],
   "source": [
    "## write function here\n",
    "def string2float(a_string):\n",
    "    '''\n",
    "    Provide a string number to convert it to a float rounded 2 places\n",
    "    '''\n",
    "    a_string = a_string.replace(\"$\", \"\").replace(\",\", \"\")  ## remove $ sign and comma\n",
    "    return round(float(a_string), 2) ## return what is converted to float and rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a724a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.24"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## call it on \"$12.24\"\n",
    "string2float(\"$12.24\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e7ddf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10201.77"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## call it on \"10,201.7654\"\n",
    "string2float(\"10,201.7654\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564db337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12501.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## call it on \"$12,501\"\n",
    "string2float(\"$12,501\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ccf1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
