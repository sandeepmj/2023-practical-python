{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rg5ypHYQ2vB"
   },
   "source": [
    "# Multi-page Tables Scrape Demo\n",
    "\n",
    "You're often going to encounter data and tables spread across hundreds if not thousands of pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLy-Ml41_-Zd"
   },
   "source": [
    "We might want to, for example, compile details about all the doctors  <a href=\"https://apps.health.ny.gov/pubdoh/professionals/doctors/conduct/factions/AllRecordsAction.action\">on this site</a> and export to a ```dataframe``` and a ```.csv``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FAa4EkaQ2vH"
   },
   "source": [
    "#### Today in class\n",
    "\n",
    "We're going to scrape as a demo a table that runs across several pages on [this mock website](https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html).\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTY4tDkVQ2vI"
   },
   "source": [
    "To capture your target information into a single CSV file will require the use of many of the foundational skills we've covered, including:\n",
    "\n",
    "- ```delays```\n",
    "- ```conditional logic```\n",
    "- ```for loops```\n",
    "\n",
    "\n",
    "And we'll explore a few new functional Python methods today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzHxjNFUQ2vI"
   },
   "source": [
    "## Scraping Strategies\n",
    "\n",
    "- How do we approach this scrape?\n",
    "- What pattern do we see?\n",
    "- How do we capture a table on a single page?\n",
    "- How do we capture a sequence of tables?\n",
    "- How we navigate from page 1 to the subsequent pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9IgexKhQ2vJ"
   },
   "source": [
    "# Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "tZ7ZNO8YQ2vJ"
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wl2yMzxEQ2vK"
   },
   "source": [
    "## Single Table Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "L3eCtCpJQ2vK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##scrape url website\n",
    "\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\"\n",
    "response = requests.get(url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PShccOFp4Xg7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## page content type\n",
    "type(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "OEbE38UkHkxb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## page text type\n",
    "type(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html lang=\"en\">\\n\\n\\t<head>\\n\\n\\t\\t<!-- Makes the page responsive and scaled to be read easily -->\\n\\t\\t<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\\n\\n\\t\\t<!-- Links to stylesheet -->\\n\\t\\t<link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">\\n\\t\\t<!-- Remember to update page title -->\\n\\t\\t<title>Heaviest Animals</title>\\n\\n\\t</head>\\n\\n\\t<body>\\n\\t\\t<!-- All content goes here -->\\n\\n\\t\\t<div class=\"container\">\\n\\t\\t\\t<section id=\"multi_table\">\\n\\t\\t\\t\\t<table class=\"full_table\">\\n\\t\\t\\t\\t\\t<h3>20 Heaviest Animals in the World</h3>\\n\\t\\t\\t\\t\\t<h4>Showing 1-5 of 20</h4>\\n\\t\\t\\t\\t\\t<thead>\\n\\t\\t\\t\\t\\t\\t<tr>\\n\\t\\t\\t\\t\\t\\t\\t<th class=\"table-head\">Animal</th>\\n\\t\\t\\t\\t\\t\\t\\t<th class=\"table-head\">Weight(kg)</th>\\n\\t\\t\\t\\t\\t\\t\\t<th class=\"table-head\">Type</th>\\n\\t\\t\\t\\t\\t\\t</tr>\\n\\t\\t\\t\\t\\t</thead>\\n\\t\\t\\t\\t\\t<tbody>\\n\\t\\t\\t\\t\\t\\t<tr>\\n\\t\\t\\t\\t\\t\\t\\t<td>Blue whale</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>136,000</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>Marine</td>\\n\\n\\t\\t\\t\\t\\t\\t</tr>\\n\\t\\t\\t\\t\\t\\t<tr>\\n\\t\\t\\t\\t\\t\\t\\t<td>Bowhead whale</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>100,000</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>Marine</td>\\n\\n\\t\\t\\t\\t\\t\\t</tr>\\n\\t\\t\\t\\t\\t\\t<tr>\\n\\t\\t\\t\\t\\t\\t\\t<td>Fin whale</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>70,000</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>Marine</td>\\n\\n\\t\\t\\t\\t\\t\\t</tr>\\n\\t\\t\\t\\t\\t\\t<tr>\\n\\t\\t\\t\\t\\t\\t\\t<td>Southern right whale</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>45,000</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>Marine</td>\\n\\n\\t\\t\\t\\t\\t\\t</tr>\\n\\t\\t\\t\\t\\t\\t<tr>\\n\\t\\t\\t\\t\\t\\t\\t<td>Humpback whale</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>30,000</td>\\n\\t\\t\\t\\t\\t\\t\\t<td>Marine</td>\\n\\n\\t\\t\\t\\t\\t\\t</tr>\\n\\t\\t\\t\\t\\t</tbody>\\n\\t\\t\\t\\t</table>\\n\\n\\t\\t\\t</section>\\n\\n\\t\\t\\t<div class=\"pagination\">\\n\\t\\t\\t\\t<a href=\"#\">&laquo;</a>\\n\\t\\t\\t\\t<a href=\"#\">1</a>\\n\\t\\t\\t\\t<a href=\"heaviest-animals-page2.html\">2</a>\\n\\t\\t\\t\\t<a href=\"heaviest-animals-page3.html\">3</a>\\n\\t\\t\\t\\t<a href=\"heaviest-animals-page4.html\">4</a>\\n\\t\\t\\t\\t<a href=\"heaviest-animals-page5.html\">5</a>\\n\\n\\t\\t\\t\\t<a href=\"heaviest-animals-page2.html\">&raquo;</a>\\n\\t\\t\\t</div>\\n\\n\\t\\t</div>\\n\\n\\n\\t</body>\\n\\n</html>'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lq7Fxh77KLU8"
   },
   "source": [
    "## ```Pandas``` captures tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gh83GGbyQ2vL"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                 Animal  Weight(kg)    Type\n",
       " 0            Blue whale      136000  Marine\n",
       " 1         Bowhead whale      100000  Marine\n",
       " 2             Fin whale       70000  Marine\n",
       " 3  Southern right whale       45000  Marine\n",
       " 4        Humpback whale       30000  Marine]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use Pandas to read tables on page\n",
    "df1 = pd.read_html(response.text)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type\n",
    "type(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## type\n",
    "type(df1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MvtGiiY7Q2vL"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animal</th>\n",
       "      <th>Weight(kg)</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue whale</td>\n",
       "      <td>136000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bowhead whale</td>\n",
       "      <td>100000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fin whale</td>\n",
       "      <td>70000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southern right whale</td>\n",
       "      <td>45000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Humpback whale</td>\n",
       "      <td>30000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Animal  Weight(kg)    Type\n",
       "0            Blue whale      136000  Marine\n",
       "1         Bowhead whale      100000  Marine\n",
       "2             Fin whale       70000  Marine\n",
       "3  Southern right whale       45000  Marine\n",
       "4        Humpback whale       30000  Marine"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Do we want the first table?\n",
    "df = df1[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dXIYAPPqQ2vL"
   },
   "outputs": [],
   "source": [
    "## store it into a copy called animals_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gheKHsbiQ2vM"
   },
   "source": [
    "## But we want to scrape multiple pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "euCu3yvc6GsF"
   },
   "outputs": [],
   "source": [
    "## Never do this manually\n",
    "url_list = [\n",
    "    \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\",\n",
    "    \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html\",\n",
    "    \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html\",\n",
    "    \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1000000.html\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46_dtNz43Cb8"
   },
   "source": [
    "## ```f-strings``` to create our links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "e8hJx1nSQ2vO"
   },
   "outputs": [],
   "source": [
    "## base url of site to scrape\n",
    "base_url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "JGQMfxyrQ2vO"
   },
   "outputs": [],
   "source": [
    "## Using a ```for loop```\n",
    "\n",
    "urls_fl = []\n",
    "for url_number in range(1,6):\n",
    "#     print(url_number)\n",
    "#     print(f\"{base_url}{url_number}.html\")\n",
    "    urls_fl.append(f\"{base_url}{url_number}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page5.html']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-WbAbHYLVcfX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page2.html',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page3.html',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page4.html',\n",
       " 'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page5.html']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using list comprehension\n",
    "urls_lc = [(f\"{base_url}{url_number}.html\") for url_number in range(1,6)]\n",
    "\n",
    "urls_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_oDySdwQ2vO"
   },
   "source": [
    "## Back to our scrape\n",
    "\n",
    "Remember, we'll hit this server too fast. We have to add a delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "RyDiu5HVQ2vQ"
   },
   "outputs": [],
   "source": [
    "## Let's import the required libaries to create a delay\n",
    "\n",
    "from random import randrange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4_Pl33BnuGck"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping 1 of 5\n",
      "<class 'list'>\n",
      "snoozing for 6 seconds before next scrape\n",
      "scraping 2 of 5\n",
      "<class 'list'>\n",
      "snoozing for 6 seconds before next scrape\n",
      "scraping 3 of 5\n",
      "<class 'list'>\n",
      "snoozing for 9 seconds before next scrape\n",
      "scraping 4 of 5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No tables found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m---> 10\u001b[0m dfL \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(response\u001b[38;5;241m.\u001b[39mtext)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(dfL))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/html.py:1205\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links)\u001b[0m\n\u001b[1;32m   1201\u001b[0m validate_header_arg(header)\n\u001b[1;32m   1203\u001b[0m io \u001b[38;5;241m=\u001b[39m stringify_path(io)\n\u001b[0;32m-> 1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/html.py:1006\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1005\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[1;32m   1008\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/html.py:986\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(io, compiled_match, attrs, encoding, displayed_only, extract_links)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 986\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/html.py:262\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/html.py:618\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[0;34m(self, doc, match, attrs)\u001b[0m\n\u001b[1;32m    615\u001b[0m tables \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mfind_all(element_name, attrs\u001b[38;5;241m=\u001b[39mattrs)\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tables:\n\u001b[0;32m--> 618\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo tables found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    620\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    621\u001b[0m unique_tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: No tables found"
     ]
    }
   ],
   "source": [
    "## first time scrape\n",
    "\n",
    "total_links = len(urls_lc)\n",
    "counter = 1\n",
    "\n",
    "for url in urls_lc:\n",
    "    print(f\"scraping {counter} of {total_links}\")\n",
    "    counter += 1\n",
    "    response = requests.get(url)\n",
    "    dfL = pd.read_html(response.text)\n",
    "    df = pd.read_html(response.text)[0]\n",
    "    print(type(dfL))\n",
    "#     print(df)\n",
    "    snoozer = randrange(5,12)\n",
    "    print(f\"snoozing for {snoozer} seconds before next scrape\")\n",
    "    time.sleep(snoozer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25AH8Ema5FT9"
   },
   "outputs": [],
   "source": [
    "## what was our status code when it broke?\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0uPWDrxJQ2vQ"
   },
   "source": [
    "## Working Around Errors\n",
    "\n",
    "When you scrape hundreds of pages, there's chance that one of the URLs might be a dud.\n",
    "\n",
    "We can set up a error control to see what kind of responses we get:\n",
    "\n",
    "```<Response [200]>``` means website is accessible.\n",
    "\n",
    "```<Response [404]>``` means broken link or no page on content.\n",
    "\n",
    "In that case, your whole code might break and you'll have to figure out where it broke.\n",
    "\n",
    "We can make that easier with ```Conditional Logic``` or ```Error Exceptions```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO50-gA84tMe"
   },
   "source": [
    "### Bypassing exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25bbP3YKvYQy"
   },
   "outputs": [],
   "source": [
    "## deal with exceptions\n",
    "## hold on to broken links\n",
    "\n",
    "busted_links = []\n",
    "df_all = []\n",
    "total_links = len(urls_lc)\n",
    "counter = 1\n",
    "\n",
    "for url in urls_lc:\n",
    "    print(f\"Scraping {counter} of {total_links}\")\n",
    "    counter += 1\n",
    "    response = requests.get(url)\n",
    "    try:\n",
    "        df = pd.read_html(response.text)[0]\n",
    "    except:\n",
    "        print(f\"oh no! {url} returned {response.status_code}\")\n",
    "        busted_links.append(url)\n",
    "    else:\n",
    "        df_all.append(df)\n",
    "        print(df)\n",
    "    snoozer = randrange(5,7)\n",
    "    print(f\"snoozing for {snoozer} seconds before next scrape\")\n",
    "    time.sleep(snoozer)\n",
    "    \n",
    "print(\"done scraping all provided links\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQ1-n--sQH9f"
   },
   "outputs": [],
   "source": [
    "## which link broke?\n",
    "busted_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_qXuB6dQkWn"
   },
   "outputs": [],
   "source": [
    "## what does df_all hold?\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cadFBbKfxoGg"
   },
   "outputs": [],
   "source": [
    "## convert to a single df rather than a list of df\n",
    "df = pd.concat(df_all, ignore_index = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsXNgC9G56a1"
   },
   "outputs": [],
   "source": [
    "## export to csv\n",
    "df.to_csv(\"heaviest_animals.csv\", index = False, encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "provenance": [
    {
     "file_id": "133K0NfwJ7esE3C-1QJ87ioTys44QdCLg",
     "timestamp": 1686746316714
    },
    {
     "file_id": "1LoyGXck4gm9F5OCk8k8YnltY-qlS7IR2",
     "timestamp": 1686738505513
    },
    {
     "file_id": "19JXSG8YXI8ZeNhzg8eaosuUkNaOp0xul",
     "timestamp": 1649162440933
    },
    {
     "file_id": "1KakOxScjK7egFn3ooKIf7Rb6ZUXqQgxK",
     "timestamp": 1645576583652
    },
    {
     "file_id": "1z2FLWwyi08NbTWdTLDEvMFbVIUWVuA7P",
     "timestamp": 1627431398775
    },
    {
     "file_id": "1VYOVnnddMwivH0B0hVpZswz7cKoEAfh-",
     "timestamp": 1627392451278
    },
    {
     "file_id": "1l6IbXo8xXwYbyNHHaNoH7QP3ssh_xrKq",
     "timestamp": 1619610819329
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
